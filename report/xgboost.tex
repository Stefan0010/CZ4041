\section{XGBoost}
XGBoost\footnote{\href{https:https://github.com/dmlc/xgboost}{XGBoost} is developed by DMLC (Distributed Machine Learning Community)} is a short name for Extreme Gradient Boosting, which is a library designed and optimized for the gradient boosting algorithms. It is an extension for classic gradient boosting algorithm. XGBoost utilizes multi-threads and regularization, making it able to do more computational power and get more accurate prediction. XGBoost is used for supervised learning problems. Since XGBoost uses tree ensembles as its model, XGBoost is pretty similar to random forests. The difference is on how they train their data. Python interface for XGBoost is used to do the Rossmann store sales. \\ \\
The general idea for this approach is just to do a comparison between our selected machine learning models and XGBoost. That is, how would general machine learning models, such as Lasso, Ridge, LSTM, and Random Forests fare against a boosting model, which in this case is XGBoost. There is no intention on using XGBoost as our final results.\\ \\
There are additional preprossessing steps for the training data before it is trained on XGBoost model. In the beginning, \textit{Stores} rows that are closed, and \textit{Stores} rows that are open, but not generating any sales, are excluded.Next, \textit{Date} column is splitted into \textit{Day}, \textit{Week}, \textit{Month}, \textit{Year}, and \textit{DayOfYear} columns. Any N/A data from those columns will be replaced by 0. Then,the \textit{CompetitionOpenSinceYear} and \textit{CompetitionOpenSinceMonth} columns are converted into integer instead of boolean. Similarly for \textit{Promo2SinceYear} and \textit{Promo2SinceWeek}, they are converted into float. Finally, any NAN data is converted into -1. In addition, there are several new features implemented, such as \textit{store data sales per customer per day}, \textit{store data customers per day}, and \textit{store data sales per customer per day}. \\ \\
For validation,the training data is splitted with 7:3 ratio. Linear regression is chosen as the objective for xgboost model training. For the other parameters applied, it is using xgtree as the booster option. Base eta (learning rate for xgboost model) is $0.1$. The maximum depth of the tree generated is 10. Subsamples for each tree is $0.85$ , and the fraction of columns to be randomly samples for each tree is $0.4$. Minimum sum of weight required in a child is 6. Number of rounds for training the data is 1200, and the evaluation metrics used is RMSPE. Early stopping value is 150. The training duration is approximately an hour.\\ \\ 
Table \ref{tab:xgboost_result} shows the performance of this xgboost model according to Kaggle evaluation. 
\begin{table}[h]
	\centering
	\caption{XGBoost result}
	\label{tab:xgboost_result}
	\begin{tabular}{|m{100pt}|m{50pt}|}
		\hline
		Private score & 0.11677 \\ \hline
		Public score  & 0.10437 \\ \hline
	\end{tabular}
\end{table}	
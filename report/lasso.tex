\section{Lasso Regression} \label{sec:lasso}
Besides neural network, we explored linear regression methods to solve the given challenge as well.

\subsection{Background}
Lasso regression is an extension of Ordinary Least Square (OLS) regression. The most notable difference between Lasso and OLS is the existence of L1 regularization term in Lasso.
\begin{equation}
\label{eq:l1_regularization}
\lambda \sum_{i=0}^{N} |w_i|
\end{equation}
The L1 regularization term is defined as the summation of the absolute of the weight for each features, multiplied by the penalty weight of $\lambda$ (default value 0.1). The L1 regularization term is included to the error term in OLS regression. The objective of defining a regularization term is to constraint the value of w in order to avoid overfitting the model.

\subsection{Implementation}
The implementation of Lasso regression model is done using Scikit Learn Library in Python, along with supporting libraries such as Pandas and Numpy.

\subsection{Preprocessing}
The data used in this experiment is derived from preprocessed data in chapter \ref{ch:preprocessing} of this report, where \textit{StateHoliday, Storetype, Assortment} are all converted into one hot encoding.\\
Additionally, \textit{CompetitionDistance} feature is normalized using min-max normalization method. \textit{Customers} data is dropped from training set as it does not exist in the testing data. Furthermore, \textit{Date} feature is also dropped from training data as we want to ensure that all features are in the form of integer, boolean, or float.

\subsection{Model Training \& Testing}
During the training and testing phase, the Lasso model was trained with different training methods, and the difference was observed based on the model's performance in the competition leaderboard. In addition, prediction is only done for open stores (i.e. \textit{Open} equals to true). For closed stores, the forecasted sales are defaulted to the value of 0.

\subsubsection{Normalized Train Test}
In this method, the \textit{Sales} column is normalized (using global mean and standard deviation of \textit{Sales}) during training and the testing result is denormalized. 
By using 20 features for training and testing, the performance of this training type, measured with RMSPE by Kaggle, is shown in table \ref{tab:lasso_norm_result}
\begin{table}[h]
	\centering
	\caption{Normalized Lasso Result}
	\label{tab:lasso_norm_result}
	\begin{tabular}{|m{100pt}|m{50pt}|}
		\hline
		Private score & 0.24082 \\ \hline
		Public score  & 0.22302 \\ \hline
	\end{tabular}
\end{table}

\subsubsection{Train Test per Store}
In this experiment, the \textit{Sales} data is normalized with regards to the mean and standard deviation of each store instead of using global mean and standard deviation. Furthermore, a unique predictive model is defined for each \textit{StoreId}.\\
The result of this model can be observed in table \ref{tab:lasso_result_per_store}.

\begin{table}[h]
	\centering
	\caption{Lasso Result per Store}
	\label{tab:lasso_result_per_store}
	\begin{tabular}{|m{100pt}|m{50pt}|}
		\hline
		Private score & 0.23920 \\ \hline
		Public score  & 0.23479 \\ \hline
	\end{tabular}
\end{table}

\subsubsection{K-Fold Cross Validation}
In addition to training and testing per store, this step performs a 5-fold validation on each predictive model during the training phase. Using the model with the best validation score, the model is then used to predict the sales of the respective store.

\begin{table}[h]
	\centering
	\caption{5-Fold Validation Lasso Result}
	\label{tab:lasso_result_5_fold}
	\begin{tabular}{|m{100pt}|m{50pt}|}
		\hline
		Private score & 0.25260 \\ \hline
		Public score  & 0.25426 \\ \hline
	\end{tabular}
\end{table}

\subsubsection{Variations of Penalty Weight ($\lambda$)}
The experiments above are repeated using different $\lambda$ values, such as $10^{-15}$, $10^{-10}$, $10^{-8}$, $10^{-4}$, $10^{-3}$, 0.01, 1, 5, 10, 20.
However, all variation of $\lambda$ produced similar or worse result compared with the default $\lambda$ value of 0.1.
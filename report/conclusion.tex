\chapter{Conclusion}
In the previous chapters, the details of each machine learning model have been discussed and explained thoroughly. All in all, LSTM per store model and NN model are not performing well. This is due to the \textit{hardware limitation} for training the models since we could not increase the \textit{number of hidden units} in LSTM model or \textit{hidden layers} in NN model. From the results gathered, it can be seen that Random Forest performs better than Neural Network, LSTM per store, Lasso and Ridge models, in terms of \textit{RMSPE}. As it has been mentioned before, \textit{XGBoost} is included just as a comparison to see how well the other models perform against boosting model. \\ \\
The best \textit{RMSPE} for Random Forest can be achieved by \textit{converting sales to its logarithmic values}, and by using \textit{30 trees} and 0.7 as \textit{maximum features}. With those features, Random Forest model can achieve $0.14692$ for private score, and $0.13378$ for public score.  
